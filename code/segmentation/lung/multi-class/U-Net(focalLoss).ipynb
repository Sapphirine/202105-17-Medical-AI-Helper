{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vital-labor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clear-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hispanic-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import normalize\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.compat.v1 import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composite-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images, if needed\n",
    "SIZE_X = 352 \n",
    "SIZE_Y = 352\n",
    "n_classes=3 #Number of classes for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "parliamentary-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture training image info as a list\n",
    "train_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "departmental-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory_path in glob.glob(\"/home/gxt/study/LungSegmentation/code/U-NetYouTube-Copy1/Imgs/Imgs24bits\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
    "        #print(img_path)\n",
    "        img = cv2.imread(img_path, 0)       \n",
    "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "        train_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wooden-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list to array for machine learning processing        \n",
    "train_images = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "athletic-agent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advisory-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "essential-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [] \n",
    "for directory_path in glob.glob(\"/home/gxt/study/LungSegmentation/code/U-NetYouTube-Copy1/GT\"):\n",
    "    for mask_path in glob.glob(os.path.join(directory_path, \"*.png\")):\n",
    "        #print(mask_path)\n",
    "        mask = cv2.imread(mask_path, 0)       \n",
    "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
    "        train_masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oriented-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list to array for machine learning processing          \n",
    "train_masks = np.array(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continuous-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tribal-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gxt/anaconda3/envs/TF20GPU/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "friendly-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images = normalize(train_images, axis=1)\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subset of data for quick testing\n",
    "#Picking 10% for testing and remaining for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
    "\n",
    "#Further split training data t a smaller subset for quick testing of models\n",
    "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"Class values in the dataset are ... \", np.unique(y1))  # 0 is the background/few unlabeled \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "#train_masks_cat = to_categorical(y1, num_classes=n_classes)\n",
    "#y_train_cat = train_masks_cat.reshape((y1.shape[0], y1.shape[1], y1.shape[2], n_classes))\n",
    "\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))\n",
    "print(X_train.shape)\n",
    "print(y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_do_not_use_masks_cat = to_categorical(y_do_not_use, num_classes=n_classes)\n",
    "y_do_not_use_cat = y_do_not_use_masks_cat.reshape((y_do_not_use.shape[0], y_do_not_use.shape[1], y_do_not_use.shape[2], n_classes))\n",
    "print(X_do_not_use.shape)\n",
    "print(y_do_not_use_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-potter",
   "metadata": {},
   "source": [
    "## Focal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "governing-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_category_focal_loss1(alpha, gamma=2.0):\n",
    "    epsilon = 1.e-7\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    alpha = tf.constant([[1],[2],[5]], dtype=tf.float32)\n",
    "    #alpha = tf.constant_initializer(alpha)\n",
    "    gamma = float(gamma)\n",
    "    def multi_category_focal_loss1_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.compat.v1.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "        fl = tf.matmul(tf.multiply(weight, ce), alpha)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    return multi_category_focal_loss1_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "further-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "def multi_unet_model(n_classes=3, IMG_HEIGHT=352, IMG_WIDTH=352, IMG_CHANNELS=1):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-relations",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.utils import class_weight\n",
    "#class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 #np.unique(train_masks_reshaped_encoded),\n",
    "                                                 #train_masks_reshaped_encoded)\n",
    "#print(\"Class weights are...:\", class_weights)\n",
    "IMG_HEIGHT = X1.shape[1]\n",
    "IMG_WIDTH  = X1.shape[2]\n",
    "IMG_CHANNELS = X1.shape[3]\n",
    "INIT_LR = 1e-6    # This value is specific to what model is chosen: Inception, VGG or ResNet etc.\n",
    "EPOCHS = 150\n",
    "#opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "def get_model():\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer=\"adam\", loss= multi_category_focal_loss1(gamma=2., alpha=.25), metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "#If starting with pre-trained weights. \n",
    "#model.load_weights('???.hdf5')\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train_cat, \n",
    "                    batch_size = 12,\n",
    "                    verbose=1,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(X_do_not_use,y_do_not_use_cat ), \n",
    "                    shuffle=False) \n",
    "                             \n",
    "                            #class_weight=class_weights,\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vulnerable-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('testOriginalImgFocalLoss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "trying-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 30ms/step\n",
      "Accuracy is =  92.01664328575134 %\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate(X_test, y_test_cat,batch_size=1)\n",
    "print(\"Accuracy is = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spanish-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('testOriginalImgFocalLoss.h5')  \n",
    "#model.load_weights('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')  \n",
    "\n",
    "#IOU\n",
    "y_pred=model.predict(X_test,batch_size=1)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-access",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 3\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "#class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "#class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "#class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "#class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[1,0]+ values[2,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] +  values[0,1]+ values[2,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1]  + values[0,2]+ values[1,2])\n",
    "#class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "#print(\"IoU for class4 is: \", class4_IoU)\n",
    "\n",
    "plt.imshow(train_images[0, :,:,0], cmap='gray')\n",
    "plt.imshow(train_masks[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-camel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import random\n",
    "#test_img_number = random.randint(0, len(X_test)-1)\n",
    "#print(test_img_number)\n",
    "\n",
    "for test_img_number in range(len(X_test)):\n",
    "    test_img = X_test[test_img_number]\n",
    "    ground_truth=y_test[test_img_number]\n",
    "    test_img_norm=test_img[:,:,0][:,:,None]\n",
    "    test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "    prediction = (model.predict(test_img_input))\n",
    "    predicted_img=np.argmax(prediction, axis=3)[0,:,:]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "    plt.imshow(ground_truth[:,:,0], cmap='jet')\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(predicted_img, cmap='jet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-keeping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
